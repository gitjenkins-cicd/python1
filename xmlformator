from bs4 import BeautifulSoup
import csv
with open('sample.xml', 'r') as f:
    data = f.read()
Bs_data = BeautifulSoup(data, "xml")
#datalist=["responseHeader","responseData"]
#for header in datalist:
b_unique = Bs_data.find_all('responseHeader')

#for x in b_unique:
 # print(x)
#print(b_unique)
with open('responseheader.csv', 'w') as f:
  write = csv.writer(f)
  for x in b_unique:
    write.writerow(x)


https://drive.google.com/file/d/1yS-E9dg-clCWSBn7tIli2syHX-U31npf/view?usp=sharing

var1=$(date '+%Y-%m-%d %H:%M:%S'|awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }')
sleep 20
var2=$(date '+%Y-%m-%d %H:%M:%S'|awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }')
diff=$(expr $var2 - $var1)
m=60
min=$(bc <<<"scale=2; $diff/$m")


sed -i 's/httpsample/buildid": "'$build'", "httpsample/' outfile1.ndl

sed 's/testResults version="1.2"/testResults version="1.2" buildid: "'$buildid'"/' error.xml

Bigquery Timepartition

https://medium.com/google-cloud/partition-on-any-field-with-bigquery-840f8aa1aaab
https://levelup.gitconnected.com/optimizing-your-bigquery-tables-using-partitioning-time-unit-column-partitioned-tables-c93cfbf3828d
https://hevodata.com/learn/bigquery-partition/
https://fivetran.com/docs/destinations/bigquery/partition-table

Bigquery insert command:

INSERT INTO `shakeersreboot.sre_dataset.partitionedtable` (Date1,msg,url,responsecode,errormsg,connect,idletime,delaytime)
SELECT Date,msg,url,responsecode,errormsg,connect,idletime,delaytime
FROM withoutpartition

Big query Alter command
ALTER TABLE IF EXISTS `shakeersreboot.sre_dataset.srepartition`
SET OPTIONS(
    require_partition_filter = false
)

partition check
SELECT * FROM `shakeersreboot.sre_dataset.INFORMATION_SCHEMA.PARTITIONS`
WHERE table_name = 'testpartition';

sed -i 's/rc="Non HTTP response code: javax.net.ssl.SSLException"/rc="400"/g'  input.csv

Taurus
 python3 -m pip install --upgrade pip setuptools wheel
 python3 -m pip install bzt
/usr/local/bin/bzt my-config.yml

test-config.yml
---
execution:
- concurrency: 10
  ramp-up: 1m
  hold-for: 1m30s
  scenario: simple

scenarios:
  simple:
    think-time: 0.75
    requests:
    - http://blazedemo.com/
    - http://blazedemo.com/vacation.html

Pod commands

kubectl cp mc1:/usr/local/ . -c 2nd

kubectl cp tomcat mc1:/usr/local -c 1st

perf metrics:
perfReport filterRegex: '', showTrendGraphs: true, sourceDataFiles: ''

New test-config.yml
---
execution:
- concurrency: 10
  ramp-up: 1m
  hold-for: 1m30s
  loop: forever
  throughput: 1
  scenario: simple

scenarios:
  simple:
    think-time: 0.75
    requests:
    - http://blazedemo.com/
    - http://blazedemo.com/vacation.html
    - http://blazedemo.com/vacati.html
settings:
  artifacts-dir: ${WORKSPACE}/${BUILD_NUMBER}
  aggregator: consolidator
  default-executor: jmeter
  check-interval: 1


jmeter manager plugin
http://search.maven.org/remotecontent?filepath=kg/apc/jmeter-plugins-cmn-jmeter/0.7/jmeter-plugins-cmn-jmeter-0.7.jar

docker run -e f="shakeer" -e a=34 -e u="shak"   flagimage:v1

kubectl get pods --no-headers -o custom-columns=":metadata.name"

split files:
#/bin/bash
split -dl 500 --additional-suffix=.csv masterbzt.log kpi
ls -p kpi* | grep -v / > fileinput.csv
for i in $(cat fileinput.csv)
do
echo "Loading file to BQ:," $i
done

https://github.com/QAInsights/Learn-Locust-Series

pod script
#/bin/bash

echo "please enter no of pods: "
read input
#for podnum in {1..'$input'}
for podnum in $(eval echo "{1..$input}")
do
cp nginx-pod.yml $podnum-nginx-pod.yml
sed -i 's/podname/nginx-'$podnum'/' $podnum-nginx-pod.yml
echo $podnum
kubectl apply -f $podnum-nginx-pod.yml
done

infinite loop

for (( ; ;))
do
var1=`tail -1 testlog`
echo $var1
if [ "$var1" == "endofscript" ]
then
echo "match found"
break;
else
echo "Under infinite loop no match found"
fi
sleep 2
done

